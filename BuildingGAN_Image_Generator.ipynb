{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ce9093f-1ba4-44de-b10b-d555e269b315",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fe42855-49b0-4d4f-bfc6-f954f4dbde4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e21630d-56c0-46ab-b02d-5565e73d30a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pre_images(directory, df):\n",
    "    disaster_counts = {disaster: 0 for disaster in df['disaster'].unique()}\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.png'):\n",
    "            parts = filename.split('_')\n",
    "            if len(parts) == 2:\n",
    "                uid, stage = parts\n",
    "                stage = stage.split('.')[0]\n",
    "                if stage == \"pre\":\n",
    "                    disaster_type = df.loc[df['uid'] == uid, 'disaster'].values\n",
    "                    if len(disaster_type) > 0:\n",
    "                        disaster_type = disaster_type[0]\n",
    "                        disaster_counts[disaster_type] += 1\n",
    "\n",
    "    for disaster, count in disaster_counts.items():\n",
    "        print(f\"Disaster type: {disaster}, Pre images: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d87f6f5-a694-4186-a2be-3b855fc70ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_post_images(directory, df):\n",
    "    disaster_counts = {disaster: 0 for disaster in df['disaster'].unique()}\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.png'):\n",
    "            parts = filename.split('_')\n",
    "            if len(parts) == 2:\n",
    "                uid, stage = parts\n",
    "                stage = stage.split('.')[0]\n",
    "                if stage == \"post\":\n",
    "                    disaster_type = df.loc[df['uid'] == uid, 'disaster'].values\n",
    "                    if len(disaster_type) > 0:\n",
    "                        disaster_type = disaster_type[0]\n",
    "                        disaster_counts[disaster_type] += 1\n",
    "\n",
    "    for disaster, count in disaster_counts.items():\n",
    "        print(f\"Disaster type: {disaster}, Post images: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d220368-9431-4cac-9fe9-0958b70128a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './tier1/cropped_square_buildings/'\n",
    "df = pd.read_csv('building_polygons_metadata.csv')\n",
    "count_pre_images(directory, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b287295-1f23-4161-bc4a-50306170d03f",
   "metadata": {},
   "source": [
    "# Image Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe5db642-fa6e-445c-9679-2dd405e79e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import tifffile as tiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29a5daa7-3b3b-44f6-b322-a42e3383f4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1, stride=1),\n",
    "            nn.InstanceNorm2d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1, stride=1),\n",
    "            nn.InstanceNorm2d(channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.conv_block(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_channels=9):  # 3 for RGB + 6 for disaster label ## CHANGE FOR DIFFERENT LABELING TECHNIQUE\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        # Layer 1\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 64, kernel_size=7, padding=3, stride=1),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Layer 2\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=4, padding=1, stride=2),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Layer 3\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=4, padding=1, stride=2),\n",
    "            nn.InstanceNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Layers 4-9: Residual blocks\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(256) for _ in range(6)]\n",
    "        )\n",
    "        \n",
    "        # Layer 10: Deconv\n",
    "        self.layer10 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Layer 11: Deconv\n",
    "        self.layer11 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Layer 12: Output layer\n",
    "        self.layer12 = nn.Sequential(\n",
    "            nn.Conv2d(64, 3, kernel_size=7, padding=3, stride=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, disaster_label):\n",
    "        # Concatenate image with label\n",
    "        disaster_label = disaster_label.view(disaster_label.size(0), -1, 1, 1)\n",
    "        disaster_label = disaster_label.repeat(1, 1, x.size(2), x.size(3))\n",
    "        x = torch.cat([x, disaster_label], dim=1)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.residual_blocks(x)\n",
    "        x = self.layer10(x)\n",
    "        x = self.layer11(x)\n",
    "        x = self.layer12(x)\n",
    "        \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13085c18-d673-4b6b-9288-ad0de1438629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_image(pre_image, disaster_onehot, checkpoint_path, device):\n",
    "    generator = Generator().to(device)\n",
    "    \n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        raise FileNotFoundError(f\"Checkpoint not found at {checkpoint_path}\")\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "    generator.eval()\n",
    "\n",
    "    pre_image = pre_image.to(device).unsqueeze(0)\n",
    "    disaster_onehot = disaster_onehot.to(device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fake_image = generator(pre_image, disaster_onehot)\n",
    "\n",
    "    pre_image_np = pre_image.squeeze(0).cpu().permute(1, 2, 0).numpy()\n",
    "    pre_image_np = (pre_image_np * 255).clip(0, 255).astype('uint8')\n",
    "\n",
    "    fake_image_np = fake_image.squeeze(0).cpu().permute(1, 2, 0).numpy()\n",
    "    fake_image_np = (fake_image_np * 255).clip(0, 255).astype('uint8')\n",
    "\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(pre_image_np)\n",
    "    plt.title(\"Original Pre Image\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(fake_image_np)\n",
    "    plt.title(\"Generated Fake Image\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return fake_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b98184a9-95ab-4589-9c88-383dca4ed607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pre_image(image_path):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    return transform(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33084804-3ecd-4063-b828-8040ddbc5c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_disaster(disaster_name):\n",
    "    \n",
    "    disaster_to_idx = {\n",
    "        'flooding': 0,\n",
    "        'wind': 1,\n",
    "        'earthquake': 2,\n",
    "        'tsunami': 3,\n",
    "        'fire': 4,\n",
    "        'volcano': 5\n",
    "    }\n",
    "    if disaster_name not in disaster_to_idx:\n",
    "        raise ValueError(f\"Invalid disaster name: {disaster_name}. Must be one of {list(disaster_to_idx.keys())}\")\n",
    "    \n",
    "    index = disaster_to_idx[disaster_name]\n",
    "    one_hot = torch.zeros(len(disaster_to_idx))\n",
    "    one_hot[index] = 1.0\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f6ad648-5d89-40b3-81fe-0bcf1f618b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_generated_image(generated_image, output_path):\n",
    "    generated_image = generated_image.squeeze(0).cpu()\n",
    "    generated_image = transforms.ToPILImage()(generated_image)\n",
    "\n",
    "    generated_image.save(output_path)\n",
    "    print(f\"Saved generated image: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7b88c92-56a5-4387-9492-4080d6b87774",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'checkpoints/latest.pth'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c12e0e-b2b2-4cb6-a71d-9632ab5f381f",
   "metadata": {},
   "source": [
    "# Rebalancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c743b56a-2e45-42e6-a59f-26dd9e381088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebalance_disaster_classes(directory, df, checkpoint_path, output_directory, device='cuda'):\n",
    "    # Get the post image counts for each disaster type\n",
    "    post_counts = df[df['stage'] == 'post'].groupby('disaster').size()\n",
    "    max_post_count = post_counts.max()\n",
    "\n",
    "    # Generate the list of filenames for pre-images from the DataFrame\n",
    "    df['filename'] = df['uid'].astype(str) + \"_\" + df['stage'].astype(str) + \".png\"\n",
    "    \n",
    "    # Get all filenames from the directory that end with '.png'\n",
    "    all_files = [f for f in os.listdir(directory) if f.endswith('.png')]\n",
    "    \n",
    "    # Filter the DataFrame for matching filenames\n",
    "    df_filtered = df[df['filename'].isin(all_files)]\n",
    "\n",
    "    # Generate a dictionary of pre-image filenames for each disaster\n",
    "    pre_images = {disaster: [] for disaster in df['disaster'].unique()}\n",
    "    \n",
    "    # Populate the pre_images dictionary with filenames for pre-images\n",
    "    for filename in all_files:\n",
    "        parts = filename.split('_')\n",
    "        if len(parts) == 2:\n",
    "            uid, stage = parts\n",
    "            stage = stage.split('.')[0]\n",
    "            if stage == \"pre\":\n",
    "                disaster_type = df.loc[df['uid'] == uid, 'disaster'].values[0]\n",
    "                pre_images[disaster_type].append(filename)\n",
    "    \n",
    "    # Generate fake post images for underrepresented disaster types\n",
    "    for disaster_type, count in post_counts.items():\n",
    "        if count < max_post_count:\n",
    "            # Collect pre-images from other disaster types\n",
    "            available_pre_images = [\n",
    "                filename for other_disaster, filenames in pre_images.items() \n",
    "                if other_disaster != disaster_type for filename in filenames\n",
    "            ]\n",
    "            \n",
    "            needed_fake_images = max_post_count - count\n",
    "            print(f\"Generating {needed_fake_images} fake post images for {disaster_type} using pre images from other disasters.\")\n",
    "\n",
    "            for i in range(needed_fake_images):\n",
    "                pre_image_filename = available_pre_images[i % len(available_pre_images)]\n",
    "                pre_image_path = os.path.join(directory, pre_image_filename)\n",
    "                generated_image = generate_fake_image(pre_image_path, disaster_type, checkpoint_path, device)\n",
    "                \n",
    "                # Extract the 'uid' from the pre-image filename\n",
    "                uid = pre_image_filename.split('_')[0]\n",
    "                fake_post_image_filename = f\"{uid}_post_{disaster_type}.png\"\n",
    "                \n",
    "                output_path = os.path.join(output_directory, fake_post_image_filename)\n",
    "                save_generated_image(generated_image, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "268a665e-2635-4f48-88df-147e849a5fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x14db6b02d100>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/ext3/miniconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m checkpoint_path = \u001b[33m'\u001b[39m\u001b[33mcheckpoints/latest.pth\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      4\u001b[39m output_directory = \u001b[33m'\u001b[39m\u001b[33m./tier1/BulidingGAN_Generated/BuildingGAN_latest/\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m rebalance_disaster_classes(directory, df, checkpoint_path, output_directory)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mrebalance_disaster_classes\u001b[39m\u001b[34m(directory, df, checkpoint_path, output_directory, device)\u001b[39m\n\u001b[32m     23\u001b[39m         stage = stage.split(\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m'\u001b[39m)[\u001b[32m0\u001b[39m]\n\u001b[32m     24\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m stage == \u001b[33m\"\u001b[39m\u001b[33mpre\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m             disaster_type = df.loc[df[\u001b[33m'\u001b[39m\u001b[33muid\u001b[39m\u001b[33m'\u001b[39m] == uid, \u001b[33m'\u001b[39m\u001b[33mdisaster\u001b[39m\u001b[33m'\u001b[39m].values[\u001b[32m0\u001b[39m]\n\u001b[32m     26\u001b[39m             pre_images[disaster_type].append(filename)\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Generate fake post images for underrepresented disaster types\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniconda3/lib/python3.12/site-packages/pandas/core/ops/common.py:76\u001b[39m, in \u001b[36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m     74\u001b[39m other = item_from_zerodim(other)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;28mself\u001b[39m, other)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniconda3/lib/python3.12/site-packages/pandas/core/arraylike.py:40\u001b[39m, in \u001b[36mOpsMixin.__eq__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m__eq__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m__eq__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._cmp_method(other, operator.eq)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniconda3/lib/python3.12/site-packages/pandas/core/series.py:6119\u001b[39m, in \u001b[36mSeries._cmp_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   6116\u001b[39m lvalues = \u001b[38;5;28mself\u001b[39m._values\n\u001b[32m   6117\u001b[39m rvalues = extract_array(other, extract_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m6119\u001b[39m res_values = ops.comparison_op(lvalues, rvalues, op)\n\u001b[32m   6121\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._construct_result(res_values, name=res_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniconda3/lib/python3.12/site-packages/pandas/core/ops/array_ops.py:344\u001b[39m, in \u001b[36mcomparison_op\u001b[39m\u001b[34m(left, right, op)\u001b[39m\n\u001b[32m    341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m lvalues.dtype == \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rvalues, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m     res_values = comp_method_OBJECT_ARRAY(op, lvalues, rvalues)\n\u001b[32m    346\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    347\u001b[39m     res_values = _na_arithmetic_op(lvalues, rvalues, op, is_cmp=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/ext3/miniconda3/lib/python3.12/site-packages/pandas/core/ops/array_ops.py:129\u001b[39m, in \u001b[36mcomp_method_OBJECT_ARRAY\u001b[39m\u001b[34m(op, x, y)\u001b[39m\n\u001b[32m    127\u001b[39m     result = libops.vec_compare(x.ravel(), y.ravel(), op)\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m     result = libops.scalar_compare(x.ravel(), y, op)\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result.reshape(x.shape)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "directory = './tier1/cropped_square_buildings/'\n",
    "df = pd.read_csv('building_polygons_metadata.csv')\n",
    "checkpoint_path = 'checkpoints/latest.pth'\n",
    "output_directory = './tier1/BulidingGAN_Generated/BuildingGAN_latest/'\n",
    "\n",
    "rebalance_disaster_classes(directory, df, checkpoint_path, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fd6cf5-2565-4fc6-8ce6-e2bbce407275",
   "metadata": {},
   "source": [
    "# Rebalanced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8467a482-9df6-428d-8b9b-84ed0f2fc8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recheck_disaster_value_counts(csv_path, original_images_dict, generated_images_dict):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    original_filenames = []\n",
    "    for disaster_type, filenames in original_images_dict.items():\n",
    "        original_filenames.extend(filenames)\n",
    "    \n",
    "    generated_filenames = []\n",
    "    for disaster_type, filenames in generated_images_dict.items():\n",
    "        generated_filenames.extend(filenames)\n",
    "    \n",
    "    # Combine both the original and generated filenames into one list\n",
    "    all_filenames = original_filenames + generated_filenames\n",
    "    \n",
    "    # Create a new column in the DataFrame for filenames\n",
    "    df['filename'] = df['uid'].astype(str) + \"_\" + df['stage'].astype(str) + \".png\"\n",
    "    \n",
    "    # Filter the DataFrame to only include rows with filenames in the all_filenames list\n",
    "    df_filtered = df[df['filename'].isin(all_filenames)]\n",
    "    \n",
    "    # Get the value counts of disaster types in the filtered DataFrame\n",
    "    disaster_counts = df_filtered['disaster'].value_counts()\n",
    "    \n",
    "    # Print the disaster counts\n",
    "    for disaster_type, count in disaster_counts.items():\n",
    "        print(f\"Disaster type: {disaster_type}, Total post images: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a640bc-5e69-4bcf-8327-6eb039ece48e",
   "metadata": {},
   "source": [
    "# Smaller dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa31bb3f-bb8a-4f5a-829a-fdd7aae88b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_disaster_counts(csv_path, image_dict):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df['filename'] = df['uid'].astype(str) + \"_\" + df['stage'].astype(str) + \".png\"\n",
    "    post_df = df[df['stage'] == 'post']\n",
    "    post_df = post_df[post_df['filename'].isin(image_dict)]\n",
    "    return post_df['disaster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "313a2c2e-42b0-44f4-ae70-fed5beb9e14f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "disaster\n",
       "wind          27466\n",
       "flooding      27293\n",
       "earthquake    22531\n",
       "tsunami       22164\n",
       "fire          17264\n",
       "volcano         767\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "directory = set(os.listdir('./tier1/cropped_square_buildings/'))\n",
    "metadata = 'building_polygons_metadata.csv'\n",
    "\n",
    "get_disaster_counts(metadata, directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be7565a5-995f-48d4-bb4f-70905a2f328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "def save_random_post_images(csv_path, directory, image_folder, output_dir='./tier1/subset/', num_images=1000):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    df['filename'] = df['uid'].astype(str) + \"_\" + df['stage'].astype(str) + \".png\"\n",
    "    post_df = df[df['stage'] == 'post']\n",
    "    post_image_files = [f for f in directory if f in post_df['filename'].values]\n",
    "\n",
    "    for disaster_type in ['wind', 'flooding', 'earthquake']:\n",
    "        disaster_df = post_df[post_df['disaster'] == disaster_type]\n",
    "        sampled_images = disaster_df.sample(n=min(num_images, len(disaster_df)), random_state=42)['filename']\n",
    "        disaster_dir = os.path.join(output_dir, disaster_type)\n",
    "        os.makedirs(disaster_dir, exist_ok=True)\n",
    "        \n",
    "        for filename in sampled_images:\n",
    "            if filename in post_image_files:\n",
    "                image_path = os.path.join(image_folder, filename)\n",
    "                dest_image_path = os.path.join(disaster_dir, filename)\n",
    "                shutil.copy(image_path, dest_image_path)\n",
    "                \n",
    "        print(f\"Saved {len(sampled_images)} images for disaster type: {disaster_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0379df0c-ab6e-42f9-9df3-893bc1d5589b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000 images for disaster type: wind\n",
      "Saved 1000 images for disaster type: flooding\n",
      "Saved 1000 images for disaster type: earthquake\n"
     ]
    }
   ],
   "source": [
    "directory = './tier1/cropped_square_buildings/'\n",
    "metadata = 'building_polygons_metadata.csv'\n",
    "num_images = 1000\n",
    "output_dir ='./tier1/subset/'\n",
    "\n",
    "save_random_post_images(metadata, directory, num_images, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "401f51a2-dc26-4b7d-a0ae-86b905471dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 500 images for disaster type: tsunami\n",
      "Saved 500 images for disaster type: fire\n",
      "Saved 500 images for disaster type: volcano\n"
     ]
    }
   ],
   "source": [
    "def save_random_post_images(csv_path, directory, image_folder, output_dir='./tier1/subset/', num_images=500):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    df = pd.read_csv(csv_path)\n",
    "    df['filename'] = df['uid'].astype(str) + \"_\" + df['stage'].astype(str) + \".png\"\n",
    "    post_df = df[df['stage'] == 'post']\n",
    "    post_image_files = [f for f in directory if f in post_df['filename'].values]\n",
    "\n",
    "    for disaster_type in ['tsunami', 'fire', 'volcano']:\n",
    "        disaster_df = post_df[post_df['disaster'] == disaster_type]\n",
    "        sampled_images = disaster_df.sample(n=min(num_images, len(disaster_df)), random_state=42)['filename']\n",
    "        disaster_dir = os.path.join(output_dir, disaster_type)\n",
    "        os.makedirs(disaster_dir, exist_ok=True)\n",
    "        \n",
    "        for filename in sampled_images:\n",
    "            if filename in post_image_files:\n",
    "                image_path = os.path.join(image_folder, filename)\n",
    "                dest_image_path = os.path.join(disaster_dir, filename)\n",
    "                shutil.copy(image_path, dest_image_path)\n",
    "                \n",
    "        print(f\"Saved {len(sampled_images)} images for disaster type: {disaster_type}\")\n",
    "\n",
    "\n",
    "directory = './tier1/cropped_square_buildings/'\n",
    "metadata = 'building_polygons_metadata.csv'\n",
    "num_images = 500\n",
    "output_dir ='./tier1/subset/'\n",
    "save_random_post_images(metadata, directory, num_images, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f54fbc92-ce57-49c7-b987-95c14f1af882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generation(csv_path, pre_image_folder, output_dir='./tier1/subset/', num_images=500):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df['filename'] = df['uid'].astype(str) + \"_\" + df['stage'].astype(str) + \".png\"\n",
    "    pre_df = df[df['stage'] == 'pre']\n",
    "\n",
    "    pre_image_files = [f for f in os.listdir(pre_image_folder) if f.endswith('.png')]\n",
    "    disaster_types = ['tsunami', 'fire', 'volcano']\n",
    "    for disaster_type in disaster_types:\n",
    "        selected_pre_images = random.sample(pre_image_files, min(num_images, len(pre_image_files)))\n",
    "        disaster_dir = os.path.join(output_dir, disaster_type)\n",
    "        os.makedirs(disaster_dir, exist_ok=True)\n",
    "\n",
    "        for filename in selected_pre_images:\n",
    "            pre_image_path = os.path.join(pre_image_folder, filename)\n",
    "            fake_image_tensor = generate_fake_image(load_pre_image(pre_image_path), one_hot_disaster(disaster_type), checkpoint_path, device)\n",
    "            \n",
    "            fake_image_filename = filename.replace('.png', '_fake.png')\n",
    "            fake_image_path = os.path.join(disaster_dir, fake_image_filename)\n",
    "            save_fake_image(fake_image_tensor, fake_image_path)\n",
    "\n",
    "def save_fake_image(fake_image_tensor, save_path):\n",
    "    if fake_image_tensor.is_cuda:\n",
    "        fake_image_tensor = fake_image_tensor.cpu()\n",
    "    fake_image_tensor = fake_image_tensor.squeeze(0)\n",
    "    fake_image_tensor = fake_image_tensor.mul(255).clamp(0, 255).byte()\n",
    "    fake_image_tensor = fake_image_tensor.permute(1, 2, 0)\n",
    "    fake_image = Image.fromarray(fake_image_tensor.numpy())\n",
    "    fake_image.save(save_path)\n",
    "    print(f\"Saved fake image: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ffdf860-3b83-4e1a-8d0b-df5bb7473bba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mrandom\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mshutil\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m generation(\u001b[39m'\u001b[39m\u001b[39mbuilding_polygons_metadata.csv\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m./tier1/cropped_square_buildings/\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generation' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "generation('building_polygons_metadata.csv', './tier1/cropped_square_buildings/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e374a6-5457-4f39-acf8-ef287724c4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
