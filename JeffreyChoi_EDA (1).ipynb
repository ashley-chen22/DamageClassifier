{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "4TjYX44-lu1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHEKjiVJlsXK"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "The funtion outputs Pandas DataFrames of the images of the training sets\n",
        "\n",
        "DataFame descriptions:\n",
        "\n",
        "data_images = {\n",
        "                id: (String) id of relevent image (same for both pre and post disaster image)\n",
        "                image_id: (String) unique id for each image\n",
        "                location: (String) disaster title\n",
        "                phase: (\"pre\" or \"post\")\n",
        "                disaster: (String) disaster done to the building\n",
        "                image_path: (String) path name to grab image later\n",
        "              }\n",
        "\n",
        "data_buildings = {\n",
        "                polygon_id: (String) unique id for each polygon annotation\n",
        "                image_id: (String) unique id for the relevent image\n",
        "                phase: (\"pre\" or \"post\")\n",
        "                damage: (String) damage type done to the building (no damage, minor damage, major damage, destroyed)\n",
        "                polygon: (Polygon Object) unique polygon outlining builiding in the image\n",
        "                 }\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def xbd_grabber(image_data_path, polygon_data_path):\n",
        "    image_entries = []\n",
        "    polygon_entries = []\n",
        "\n",
        "    for json_file in os.listdir(polygon_data_path):\n",
        "        if json_file.endswith(\".json\"):\n",
        "            with open(os.path.join(polygon_data_path, json_file), 'r') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            # Extract image-level metadata\n",
        "            image_name = data['metadata']['img_name']\n",
        "            image_path = os.path.join(image_data_path, image_name.replace(\".png\", \".tif\"))\n",
        "\n",
        "            base_name = image_name.replace(\".png\", \"\")\n",
        "            parts = base_name.split(\"_\")\n",
        "\n",
        "            id_ = data['metadata']['id']\n",
        "            image_id = parts[1]\n",
        "            phase = parts[2]\n",
        "            location = data['metadata']['disaster']\n",
        "            disaster = data['metadata']['disaster_type']\n",
        "\n",
        "            image_entries.append({\n",
        "                \"id\": id_,\n",
        "                \"image_id\": image_id,\n",
        "                \"location\": location,\n",
        "                \"phase\": phase,\n",
        "                \"disaster\": disaster,\n",
        "                \"image_path\": image_path\n",
        "            })\n",
        "\n",
        "            for feature in data['features']['xy']:\n",
        "                polygon_id = feature[\"properties\"][\"uid\"]\n",
        "                damage = \"pre\" if phase == \"pre\" else feature[\"properties\"][\"subtype\"]\n",
        "                polygon = feature[\"wkt\"]\n",
        "\n",
        "                polygon_entries.append({\n",
        "                    \"polygon_id\": polygon_id,\n",
        "                    \"image_id\": image_id,\n",
        "                    \"phase\": phase,\n",
        "                    \"damage\": damage,\n",
        "                    \"polygon\": polygon\n",
        "                })\n",
        "\n",
        "    data_images = pd.DataFrame(image_entries)\n",
        "    data_buildings = pd.DataFrame(polygon_entries)\n",
        "\n",
        "    return data_images, data_buildings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The function crops each original remote sensing image (1024 × 1024) to 16 remote sensing images (256 × 256)\n",
        "and saves to a new directory\n",
        "\"\"\"\n",
        "def crop_and_save_images(paired_images, save_dir_pre, save_dir_post):\n",
        "    os.makedirs(save_dir_pre, exist_ok=True)\n",
        "    os.makedirs(save_dir_post, exist_ok=True)\n",
        "\n",
        "    count = 0\n",
        "    for pre_path, post_path, label in paired_images:\n",
        "        pre_img = Image.open(pre_path).convert('RGB')\n",
        "        post_img = Image.open(post_path).convert('RGB')\n",
        "\n",
        "        for i in range(0, 1024, 256):\n",
        "            for j in range(0, 1024, 256):\n",
        "                pre_crop = pre_img.crop((j, i, j+256, i+256))\n",
        "                post_crop = post_img.crop((j, i, j+256, i+256))\n",
        "                crop_name = f\"{count}_label{label}.png\"\n",
        "                pre_crop.save(os.path.join(save_dir_pre, crop_name))\n",
        "                post_crop.save(os.path.join(save_dir_post, crop_name))\n",
        "                count += 1"
      ],
      "metadata": {
        "id": "idoW1FPIlwTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_images_tier1, data_buildings_tier1 = xbd_grabber(\"./geotiffs/tier1/images/\", \"./geotiffs/tier1/labels/\")\n",
        "data_images_tier3, data_buildings_tier3 = xbd_grabber(\"./geotiffs/tier3/images/\", \"./geotiffs/tier3/labels/\")\n",
        "\n",
        "data_images = pd.concat([data_images_tier1, data_images_tier3], ignore_index=True)\n",
        "data_buildings = pd.concat([data_buildings_tier1, data_buildings_tier3], ignore_index=True)"
      ],
      "metadata": {
        "id": "O7zWfJ4tnXS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Creates array of image path pairs for pre and post images \"\"\"\n",
        "paired_images = []\n",
        "df_pre = data_images[data_images['phase'] == 'pre']\n",
        "df_post = data_images[data_images['phase'] == 'post']\n",
        "merged = pd.merge(df_pre, df_post, on='id', suffixes=('_pre', '_post'))\n",
        "\n",
        "for _, row in merged.iterrows():\n",
        "    pre_path = row['image_path_pre']\n",
        "    post_path = row['image_path_post']\n",
        "    paired_images.append((pre_path, post_path))\n",
        "\n",
        "crop_and_save_images(paired_images, \"./geotiffs/cropped_training_pre\", \"./geotiffs/cropped_training_post\")"
      ],
      "metadata": {
        "id": "I7gZsyU4mVc0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}